{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-64., -56., -61., -66., -71., -82., -81.,   1.],\n",
      "        [-68., -57., -61., -65., -71., -85., -85.,   1.],\n",
      "        [-63., -60., -60., -67., -76., -85., -84.,   1.],\n",
      "        [-61., -60., -68., -62., -77., -90., -80.,   1.],\n",
      "        [-63., -65., -60., -63., -77., -81., -87.,   1.],\n",
      "        [-64., -55., -63., -66., -76., -88., -83.,   1.],\n",
      "        [-65., -61., -65., -67., -69., -87., -84.,   1.],\n",
      "        [-61., -63., -58., -66., -74., -87., -82.,   1.],\n",
      "        [-65., -60., -59., -63., -76., -86., -82.,   1.],\n",
      "        [-62., -60., -66., -68., -80., -86., -91.,   1.],\n",
      "        [-40., -56., -57., -35., -84., -70., -66.,   2.],\n",
      "        [-36., -60., -57., -37., -68., -66., -77.,   2.],\n",
      "        [-36., -59., -57., -19., -70., -69., -75.,   2.],\n",
      "        [-17., -59., -63., -15., -70., -76., -85.,   2.],\n",
      "        [-35., -57., -54., -35., -64., -78., -76.,   2.],\n",
      "        [-39., -54., -55., -37., -61., -77., -79.,   2.],\n",
      "        [-42., -56., -52., -36., -75., -68., -81.,   2.],\n",
      "        [-39., -51., -64., -38., -71., -78., -78.,   2.],\n",
      "        [-43., -53., -53., -38., -63., -76., -73.,   2.],\n",
      "        [-40., -56., -56., -38., -66., -77., -74.,   2.],\n",
      "        [-50., -60., -73., -58., -68., -88., -92.,   3.],\n",
      "        [-49., -59., -57., -50., -72., -80., -91.,   3.],\n",
      "        [-49., -59., -56., -50., -63., -80., -91.,   3.],\n",
      "        [-49., -62., -56., -50., -63., -83., -89.,   3.],\n",
      "        [-49., -61., -56., -49., -64., -84., -89.,   3.],\n",
      "        [-49., -60., -55., -49., -63., -85., -89.,   3.],\n",
      "        [-49., -60., -56., -50., -63., -84., -87.,   3.],\n",
      "        [-49., -61., -55., -49., -62., -84., -89.,   3.],\n",
      "        [-49., -61., -55., -53., -72., -85., -92.,   3.],\n",
      "        [-53., -57., -62., -55., -60., -86., -83.,   3.],\n",
      "        [-61., -55., -46., -57., -46., -83., -85.,   4.],\n",
      "        [-56., -53., -49., -60., -48., -82., -85.,   4.],\n",
      "        [-61., -56., -52., -62., -52., -82., -87.,   4.],\n",
      "        [-64., -58., -48., -63., -42., -85., -93.,   4.],\n",
      "        [-58., -56., -44., -60., -47., -84., -85.,   4.],\n",
      "        [-58., -51., -49., -61., -48., -88., -85.,   4.],\n",
      "        [-57., -55., -45., -59., -50., -85., -85.,   4.],\n",
      "        [-61., -52., -48., -57., -47., -85., -85.,   4.],\n",
      "        [-59., -50., -51., -59., -51., -83., -85.,   4.],\n",
      "        [-60., -52., -48., -61., -48., -83., -84.,   4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "#dataset taken from https://www.kaggle.com/yashsawarn/wifi-stretgth-for-rooms\n",
    "\n",
    "def read_dataset(csv_name = 'wifi_localizations.txt'):\n",
    "    \"\"\"\n",
    "    Reads a csv dataset \n",
    "    returns it as a pytorch tensor\n",
    "    \"\"\"\n",
    "    data_frame = pandas.read_table(csv_name, delim_whitespace=True, names=('A', 'B', 'C', 'D','E', 'F', 'G', 'ROOM'),\n",
    "                       dtype={'A': np.int64, 'B': np.float64, 'C': np.float64, 'D': np.float64,'E': np.float64,'F': np.float64,'G': np.float64,'ROOM': np.float64})\n",
    "\n",
    "    targets_torch = torch.tensor(data_frame['ROOM'].values)\n",
    "    dataset_torch = torch.tensor(data_frame.values)\n",
    "    print(dataset_torch)\n",
    "    return dataset_torch\n",
    "\n",
    "dataset_torch = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_thresh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-241553d2e231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_CART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_xml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"CART_example.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_CART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-241553d2e231>\u001b[0m in \u001b[0;36mtrain_CART\u001b[1;34m(dataset_torch, name_xml, max_CART_depth, min_obs_per_leaf)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m    203\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_CART_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_CART_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_observations\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmin_obs_per_leaf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_CART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mname_xml\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-241553d2e231>\u001b[0m in \u001b[0;36mbuild_CART\u001b[1;34m(self, data_torch)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mBuild\u001b[0m \u001b[0mCART\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_selected_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_with_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxml_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-241553d2e231>\u001b[0m in \u001b[0;36mcreate_with_children\u001b[1;34m(self, data_torch, current_depth, list_selected_features, min_gini)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m#careful with max depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m#if no threshold and feature were selected, select it using a greedy approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mthreshold_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgini\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_best_feature_and_thresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_features_selected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_selected_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mlist_selected_features\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeature_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m#store important data in attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-241553d2e231>\u001b[0m in \u001b[0;36mselect_best_feature_and_thresh\u001b[1;34m(self, data_torch, list_features_selected, num_classes)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m#TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m#return selected cut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmin_thresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_gini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'min_thresh' is not defined"
     ]
    }
   ],
   "source": [
    "class Node_CART:    \n",
    "    def __init__(self, num_classes = 4, ref_CART = None, current_depth = 0):\n",
    "        \"\"\"\n",
    "        Create the node attributes\n",
    "        param num_classes: K number of classes to classify\n",
    "        param ref_cart: reference to the tree containing the node\n",
    "        param current_depth: current depth of the node in the tree\n",
    "        \"\"\"\n",
    "        self.ref_CART = ref_CART\n",
    "        self.threshold_value = 0\n",
    "        self.feature_num = 0\n",
    "        self.node_right = None\n",
    "        self.node_left = None\n",
    "        self.data_torch_partition = None\n",
    "        self.gini = 0\n",
    "        self.dominant_class = None\n",
    "        self.accuracy_dominant_class = None        \n",
    "        self.num_classes = num_classes#Por que num_clases = 4\n",
    "        self.current_depth = current_depth\n",
    "    \n",
    "    def to_xml(self, current_str = \"\"):\n",
    "        \"\"\"\n",
    "        Recursive function to write the node content to an xml formatted string\n",
    "        param current_str : the xml content so far in the whole tree\n",
    "        return the string with the node content\n",
    "        \"\"\"\n",
    "        str_node = \"<node><thresh>\" + str(self.threshold_value) + \"</thresh>\" + \"<feature>\" + str(self.feature_num) + \"</feature><depth>\" + str(self.current_depth)+ \"</depth>\" \n",
    "        str_node += \"<gini>\" + str(self.gini) + \"</gini>\"\n",
    "        if(self.node_right != None):\n",
    "            str_left = self.node_right.to_xml(current_str)\n",
    "            str_node += str_left\n",
    "        if(self.node_left != None):\n",
    "            str_right = self.node_left.to_xml(current_str)\n",
    "            str_node += str_right\n",
    "            \n",
    "        if(self.is_leaf()):\n",
    "            str_node += \"<dominant_class>\" + str(self.dominant_class) + \"</dominant_class><acc_dominant_class>\"  + str(self.accuracy_dominant_class) + \"</acc_dominant_class>\"\n",
    "        str_node += \"</node>\"\n",
    "        return str_node\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Checks whether the node is a leaf\n",
    "        \"\"\"\n",
    "        return (self.node_left == None and self.node_right == None)\n",
    "    \n",
    "    def create_with_children(self, data_torch, current_depth, list_selected_features = [], min_gini = 0.000001):\n",
    "        \"\"\"\n",
    "        Creates a node by selecting the best feature and threshold, and if needed, creating its children\n",
    "        param data_torch: dataset with the current partition to deal with in the node\n",
    "        param current_depth: depth counter for the node\n",
    "        param list_selected_features: list of selected features so far for the CART building process\n",
    "        param min_gini: hyperparmeter selected by the user defining the minimum tolerated gini coefficient for a  node\n",
    "        return the list of selected features so far\n",
    "        \"\"\"        \n",
    "        #update depth of children\n",
    "        depth_children = current_depth + 1\n",
    "        if(depth_children <= self.ref_CART.get_max_depth()):\n",
    "            num_observations = data_torch.shape[0]            \n",
    "            #careful with max depth\n",
    "            #if no threshold and feature were selected, select it using a greedy approach            \n",
    "            (threshold_value, feature_num, gini) = self.select_best_feature_and_thresh(data_torch, list_features_selected = list_selected_features)\n",
    "            list_selected_features += [feature_num]\n",
    "            #store important data in attributes\n",
    "            self.threshold_value = threshold_value\n",
    "            self.feature_num = feature_num\n",
    "            self.data_torch_partition = data_torch\n",
    "            self.gini = gini            \n",
    "            num_features = data_torch.shape[1]\n",
    "            #data_torch_left = torch.zeros(1, num_features)\n",
    "            #data_torch_right = torch.zeros(1, num_features)\n",
    "            #create the right and left node data if the current gini is still high            \n",
    "            if(self.gini > min_gini):                \n",
    "                data_torch_left = data_torch[data_torch[:, feature_num] < threshold_value]\n",
    "                data_torch_right = data_torch[data_torch[:, feature_num] >= threshold_value]\n",
    "                #if the new partitions have more than min_observations, make them\n",
    "                if(data_torch_left.shape[0] >= self.ref_CART.get_min_observations() and data_torch_right.shape[0] >= self.ref_CART.get_min_observations()):\n",
    "                    #add data to the right and left children\n",
    "                    self.node_right = Node_CART(num_classes = self.num_classes, ref_CART = self.ref_CART, current_depth = depth_children)\n",
    "                    self.node_left = Node_CART(num_classes = self.num_classes, ref_CART = self.ref_CART, current_depth = depth_children)\n",
    "                    list_selected_features = self.node_right.create_with_children(data_torch_right, depth_children, list_selected_features = list_selected_features)            \n",
    "                    self.node_left.create_with_children( data_torch_left, depth_children, list_selected_features = list_selected_features)\n",
    "        #if is leaf, fill the         \n",
    "        if(self.is_leaf()):            \n",
    "            labels_data = data_torch[:,  -1]\n",
    "            self.dominant_class = torch.mode(labels_data).values.item()\n",
    "            num_obs_label = labels_data[labels_data == self.dominant_class].shape[0]\n",
    "            self.accuracy_dominant_class = num_obs_label / labels_data.shape[0]           \n",
    "            \n",
    "        return list_selected_features\n",
    "    \n",
    "    \n",
    "    def select_best_feature_and_thresh(self, data_torch, list_features_selected = [], num_classes = 4):\n",
    "        \"\"\"\n",
    "        Selects the best feature and threshold that minimizes the gini coefficient\n",
    "        param data_torch: dataset partition to analyze\n",
    "        param list_features_selected list of features selected so far, thus must be ignored \n",
    "        param num_classes: number of K classes to discriminate from \n",
    "        return min_thresh, min_feature, min_gini found for the dataset partition when \n",
    "        selecting the found feature and threshold\n",
    "        \"\"\"       \n",
    "        \n",
    "        #TODO\n",
    "        #return selected cut       \n",
    "        return (min_thresh, min_feature, min_gini)   \n",
    "        \n",
    "    \n",
    "    def calculate_gini(self, data_partition_torch, num_classes = 4):\n",
    "        \"\"\"\n",
    "        Calculates the gini coefficient for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated gini coefficient\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "         \n",
    "        return gini\n",
    "    \n",
    "    def calculate_entropy(self, data_partition_torch, num_classes = 4):\n",
    "        \"\"\"\n",
    "        Calculates the entropy for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated entropy\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "         \n",
    "        return entropy\n",
    "    \n",
    "    def evaluate_node(self, input_torch): \n",
    "        \"\"\"\n",
    "        Evaluates an input observation within the node. \n",
    "        If is not a leaf node, send it to the corresponding node\n",
    "        return predicted label\n",
    "        \"\"\"\n",
    "        feature_val_input = input_torch[self.feature_num]\n",
    "        if(self.is_leaf()):\n",
    "            return self.dominant_class\n",
    "        else:\n",
    "            if(feature_val_input < self.threshold_value):\n",
    "                return self.node_left.evaluate_node(input_torch)\n",
    "            else:\n",
    "                return self.node_right.evaluate_node(input_torch)\n",
    "        \n",
    "\n",
    "class CART:\n",
    "    def __init__(self, dataset_torch, max_CART_depth, min_observations = 2):\n",
    "        \"\"\"\n",
    "        CART has only one root node\n",
    "        \"\"\"\n",
    "        #min observations per node\n",
    "        self.min_observations = min_observations\n",
    "        self.root = Node_CART(num_classes = 4, ref_CART = self, current_depth = 0)\n",
    "        self.max_CART_depth = max_CART_depth\n",
    "        self.list_selected_features = []\n",
    "        \n",
    "    def get_root(self):\n",
    "        \"\"\"\n",
    "        Gets tree root\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "    \n",
    "    def get_min_observations(self):\n",
    "        \"\"\"\n",
    "        return min observations per node\n",
    "        \"\"\"\n",
    "        return self.min_observations\n",
    "    \n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Gets the selected max depth of the tree\n",
    "        \"\"\"\n",
    "        return self.max_CART_depth\n",
    "    \n",
    "    def build_CART(self, data_torch):\n",
    "        \"\"\"\n",
    "        Build CART from root\n",
    "        \"\"\"\n",
    "        self.list_selected_features = self.root.create_with_children(data_torch, current_depth = 0)\n",
    "    \n",
    "    def to_xml(self, xml_file_name):\n",
    "        \"\"\"\n",
    "        write Xml file with tree content\n",
    "        \"\"\"\n",
    "        str_nodes = self.root.to_xml()\n",
    "        file = open(xml_file_name,\"w+\") \n",
    "        file.write(str_nodes)\n",
    "        file.close()\n",
    "        return str_nodes\n",
    "    \n",
    "    \n",
    "    def evaluate_input(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluate a specific input in the tree and get the predicted class\n",
    "        \"\"\"\n",
    "        return self.root.evaluate_node(input_torch)\n",
    "        \n",
    "    \n",
    "def train_CART(dataset_torch, name_xml = \"\", max_CART_depth = 3, min_obs_per_leaf = 2): \n",
    "    \"\"\"\n",
    "    Train CART model\n",
    "    \"\"\"\n",
    "    tree = CART(dataset_torch = dataset_torch, max_CART_depth = max_CART_depth, min_observations =  min_obs_per_leaf)\n",
    "    tree.build_CART(dataset_torch)\n",
    "    if(not name_xml == \"\"):\n",
    "        tree.to_xml(name_xml)\n",
    "    return tree\n",
    "\n",
    "def test_CART(tree, testset_torch):\n",
    "    \"\"\"\n",
    "    Test a previously built CART\n",
    "    \"\"\"\n",
    "    #TODO, use tree.evaluate_input(current_observation) for this\n",
    "    return accuracy\n",
    "\n",
    "        \n",
    "\n",
    "tree = train_CART(dataset_torch, name_xml = \"CART_example.xml\")\n",
    "acc = test_CART(tree, dataset_torch)\n",
    "        \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Threshold tensor(-39., dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 4., 1., 1., 1., 1., 1., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 2.],\n",
      "       dtype=torch.float64)\n",
      "<class 'tuple'>\n",
      "Conteos (tensor([1., 2., 3., 4.], dtype=torch.float64), tensor([10,  5, 10, 10]))\n",
      "Conteos tensor([1., 2., 3., 4.], dtype=torch.float64)\n",
      "Conteos tensor([10,  5, 10, 10])\n",
      "Probabilidades tensor([0.2857, 0.1429, 0.2857, 0.2857])\n"
     ]
    }
   ],
   "source": [
    "DataSetQtyColumns = dataset_torch.size(1)\n",
    "QTY_Features = DataSetQtyColumns - 1\n",
    "\n",
    "\n",
    "reducedataset = dataset_torch[:,[0,7]]\n",
    "\n",
    "#reducedataset = torch.tensor([[5, 5],[5, 3],[3, 5],[6, 4],[3, 7]])\n",
    "\n",
    "def select_best_feature_and_thresh(data_partition_torch,x, num_classes = 4):\n",
    "    \"\"\"\n",
    "    Selects the best feature and threshold that minimizes the gini coefficient\n",
    "    param data_torch: dataset partition to analyze\n",
    "    param list_features_selected list of features selected so far, thus must be ignored \n",
    "    param num_classes: number of K classes to discriminate from \n",
    "    return min_thresh, min_feature, min_gini found for the dataset partition when \n",
    "    selecting the found feature and threshold\n",
    "    \"\"\"       \n",
    "    SortedRawPartition = data_partition_torch[data_partition_torch[:, 0].sort()[1]]   #Sort  data_partition_torch low to high\n",
    "    QTYofRegisters = SortedRawPartition.size(0)#Qty of Registers of data_partition_torch\n",
    "    \n",
    "   \n",
    "    print(x)\n",
    "    print(\"Threshold\",SortedRawPartition[x,0])\n",
    "    leftPartition = SortedRawPartition[:x]\n",
    "    #print(leftPartition)\n",
    "    return(leftPartition)\n",
    "    #rightPartition = SortedRawPartition[x:]\n",
    "    #print(rightPartition)\n",
    "    #print(\"***\")\n",
    "    #TODO\n",
    "    #return selected cut       \n",
    "    #return (min_thresh, min_feature, min_gini)  \n",
    "\n",
    "def calculate_entropy(data_partition_torch, num_classes = 4):\n",
    "    \"\"\"\n",
    "    Calculates the entropy for a given partition with the given number of classes\n",
    "    param data_partition_torch: current dataset partition as a tensor\n",
    "    param num_classes: K number of classes to discriminate from\n",
    "    returns the calculated entropy\n",
    "    \"\"\"\n",
    "    sizeOfPartition = data_partition_torch.size(0)\n",
    "    tags = data_partition_torch[:,1]\n",
    "    print(tags)\n",
    "    conteos = tags.unique(return_counts = True, sorted=True)\n",
    "    print(type(conteos))\n",
    "    print(\"Conteos\",conteos)\n",
    "    print(\"Conteos\",conteos[0])\n",
    "    print(\"Conteos\",conteos[1])\n",
    "    probabilidades = conteos[1]/sizeOfPartition\n",
    "    print(\"Probabilidades\", probabilidades)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "partition = select_best_feature_and_thresh(reducedataset,35)       \n",
    "    \n",
    "calculate_entropy(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
